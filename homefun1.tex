\documentclass{article}

\usepackage{listings}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{url}

\title{Homefun 1}
\author{Ben Gamari}

\begin{document}
\maketitle

\section{Question 1}

We are given a network of nodes with edges $E = \{ oa, ob, oc, ab, ad, cd,
be, ce, dn, en \}$, with maximum flow rates $c_e \forall e \in E$. We want
to determine the direction and magnitude of the flow demanded of each
edge such that the flow between $o$ and $n$ is maximized.

The variables are the flow demanded of each edge, $f_e$ such that
$|f_e| \le c_e$. However, to conveniently cast this as in our
optimization framework (which expects configuration vectors to be
positive semidefinite (PSD)), we can rewrite this as $f'_e = f_e + c_e$ such
that $0 \le f'_e \le 2 c_e$.

We demand that the net flow in to each node is zero,

\begin{align*}
  f_{oa} - f_{ab} - f_{ad} = 0 \tag{node $a$} \\
  f_{ob} + f_{ab} - f_{be} = 0 \tag{node $b$} \\
  f_{oc} - f_{cd} - f_{ce} = 0 \tag{node $c$} \\
  f_{ad} + f_{cd} - f_{dn} = 0 \tag{node $d$} \\
  f_{be} + f_{ce} - f_{en} = 0 \tag{node $e$} \\
\end{align*}

Which we can then rewrite in terms of $f'$,

\begin{align*}
  f'_{oa} - f'_{ab} - f'_{ad} = 1  \tag{node $a$} \\
  f'_{ob} + f'_{ab} - f'_{be} = -1 \tag{node $b$} \\
  f'_{oc} - f'_{cd} - f'_{ce} = -7 \tag{node $c$} \\
  f'_{ad} + f'_{cd} - f'_{dn} = 1  \tag{node $d$} \\
  f'_{be} + f'_{ce} - f'_{en} = 6  \tag{node $e$} \\
\end{align*}

Which is of form $A \vec f' = b$ where $A$ is read off of the
left-hand side and $b$ from the right.

Since we demand that the net flow in each node (other than $o$ and
$n$) is zero, we know that the net flow into $n$ is precisely equal to
the net flow out of $o$. Consequently, we can take either of these net
flows to be our objective function,

\begin{align*}
  F(\vec f) & = f_{dn} + f_{en} = - f_{oa} - f_{ob} - f_{oc} \\
            & = f'_{dn} + f'_{en} + \mathrm{const} \\
            & = - f'_{oa} - f_{ob} - f_{oc} + \mathrm{const} \\
\end{align*}

Again, this is in the form of $c^T \vec f'$ (up to constants which
can be ignored in the optimization problem).

After solving our optimization problem,

\[ \mathrm{min}_{\vec f'} ~ c^T \vec f' ~ \mathrm{s.t.} ~ A \vec f', \vec f' \ge 0 \]

We can extract our flows $f_e = f'_e - c_e$.

Mathematica tells us (where, e.g., {\tt foc} corresponds to $f'_{oc}$),

\begin{lstlisting}
In[11]:= Minimize[{
  fdn + fen,
  foa - fab - fad == 1
   && fob + fab - fbe == -1
   && foc - fcd - fce == -7
   && fad + fcd - fdn == 1
   && fbe + fce - fen == 6
   && foa >= 0 && fob >= 0 && foc >= 0
   && fab >= 0 && fad >= 0 && fcd >= 0
   && fbe >= 0 && fce >= 0 && fdn >= 0 && fen >= 0
  }, {foa, fob, foc, fab, fad, fcd, fbe, fce, fdn, fen}]

Out[11]= {1, {foa -> 1, fob -> 0, foc -> 0, fab -> 0, fad -> 0,
  fcd -> 2, fbe -> 1, fce -> 5, fdn -> 1, fen -> 0}}
\end{lstlisting}

Resulting in a bandwidth of 5 units.

\section{Question 2}

We define the configuration space of this problem to consist of the
following variables,

\begin{itemize}
  \item $x_{ij}$, the quantity each raw component $i$ used to produce blend $j$

  \item $y_i$, the quantity of each raw component $i$ to buy

  \item $z_j$, the quantity of each blend $j$ to produce and sell
\end{itemize}
%
We denote the quantities given in the problem description with the following,

\begin{itemize}
\item $b_i$, the quantity of raw component $i$ available for purchase
\item $d_j$, the maximum demand of blend $j$
\item $p_i$, the purchase price of raw component $i$
\item $p'_j$, the selling price of blend $j$
\item $e_i$, the selling price of excess of raw component $i$
\item $o_i$, the octane content of raw component $i$
\item $r_j$, the minimum octane content of blend $j$
\end{itemize}

We note that the amount of raw components $x_{ij}$ used to produce a
blend is related to the amount of the blend produced (under an
assumption of volume conservation),

\[ \sum_i x_{ij} = z_j \tag{C1} \]

The constraint on the availability of raw component $i$ can be written
as,
\[ \sum_j x_{ij} \le b_i \]
This inequality can be recast as an equality through the introduction
of a slack variable $\alpha > 0$,
\[ \sum_j x_{ij} + \alpha_i = b_i \tag{C2} \]

Likewise, the constraint on the demand for blend $j$ is written as,
\[ z_j = \sum_i x_{ij} \le d_j \]
which after introduction of the slack variable $\beta_j$ yields,
\[ \sum_i x_{ij} + \beta_j = d_j \tag{C3} \]

Lastly, we must ensure that each blend $j$ meets its minimum octane requirement,
\[ \sum_i o_i x_{ij} \ge r_j z_j \]
or equivalently,
\[ \sum_i o_i x_{ij} - r_j z_j - \gamma_j = 0 \tag{C4} \]

Our objective function can be formed from three distinct contributions.
The cost of purchasing the raw components is given by,
\[ F_1 = \sum_i p_i y_i \]
The revenue from selling the blended products is given by,
\[ F_2 = \sum_j p'_j z_j \]
The revenue from selling the excess raw components is given by,
\[ F_3 = \sum_i e_i \left(y_i - \sum_j x_{ij}\right) \]
The objective (profit) to be maximized is then given by,
\[ F = F_2 + F_3 - F_1 \tag{Obj} \]

We note that our optimization space now consists of vectors formed from
$(\vec x, \vec y, \vec z, \vec\alpha, \vec\beta, \vec\gamma) \ge 0$.

\section{Question 3}

We will need two projection methods:

\begin{enumerate}
\item A projection from a partially specified matrix symmetric matrix $M$
  to a PSD matrix $X \in S_+^n$.
\item A projection from a PSD matrix $X \in S_+^n$ to the set of
  completions of $M$.
\end{enumerate}

In both cases we will minimize the nuclear norm,
\[ \Vert X \Vert = \Vert V \Lambda V^T \Vert = \sum_i \sigma_i \]
where the singular value $\sigma_i = \vert \lambda_i \vert$.

For (a), we recognize that the symmetric matrix $X$ can be represented
in the eigenbasis of $M$, $X = V \Omega V^T$. However, $M$ may have
negative eigenvalues, which we must eliminate in order for $X$ to be
PSD. We recognize that in these cases $\Vert X \Vert$ is minimized by
letting $\omega_i = 0$ for all negative eigenvalues $\lambda_i <
0$.

For (b), we speculate that the minimal norm projection from $X$ to a
valid completion $M$ is $X$ with the observed entries placed.

This has been implemented in Python (see Section
\ref{Q3Source}). Convergence characteristics for a small,
underdetermined system is shown in Figure \ref{Fig:AltProjConv}.

\begin{figure}
  \center
  \includegraphics[scale=0.5]{q3-convergence.pdf}
  \caption{(Question 3) Convergence of method of alternating projections on 100x100
  symmetric matrix with 10 observed elements}
  \label{Fig:AltProjConv}
\end{figure}

\section{Question 4}

We expect that the random systems we generate will likely be poorly
conditioned, giving conjugate gradient a distinct advantage over
steepest descent. While Newton's method requires inversion of $A$, it
should converge extremely quickly in the case of our linear problem as
its linear model of the function fits its behavior perfectly.

Figure \ref{Fig:Q4Convergence} shows convergence of these three methods on
a randomly generated linear system of ten equations and ten
variables. The observed convergence characteristics agree with the
intuition stated about. We find that Newton's method convergences
nearly immediately down to machine precision while conjugate gradient
requires ten iterations, exactly enough to cover the ten dimensions of
the system once, before reaching convergence. As expected, steepest
descent is crippled by the poor conditioning of the system.

The source for the implementation of this method is given in Section
\ref{Q4Source}.

\begin{figure}
  \center
  \includegraphics[scale=0.5]{q4-convergence.pdf}
  \caption{(Question 4) Convergence of methods on 10 equation, 10 variable linear system}
  \label{Fig:Q4Convergence}
\end{figure}

\section{Question 5}

Convergence for the three methods on the Rosenbrock function is shown
in Figure \ref{Fig:Q5Convergence}. The source is given in Section
\ref{Q5Source}. We find that Newton's method quickly converges, as
expected. Steepest descent is slowest as it gets stuck traversing
across the function's valley as it approaching the minimum. Conjugate
gradient avoids this and converges more quickly, although it's
interesting to note that the error decreases in sudden jumps after
around 16 and 32 iterations.

Since originally writing this exercise, I have refactored it into a
library available on Github. It can be found at
\url{https://github.com/bgamari/optimization} and will soon be
incorporated into the {\tt ad} package
(\url{http://github.com/ekmett/ad}), a Haskell automatic
differentiation and optimization library.

\begin{figure}
  \center
  \includegraphics[scale=0.5]{q5.pdf}
  \caption{(Question 5) Convergence of methods Rosenbrock function up to 50 iterations}
  \label{Fig:Q5Convergence}
\end{figure}

\appendix

\section{Question 3 source}\label{Q3Source}
\lstinputlisting[language=Python]{q3.py}

\section{Question 4 source}\label{Q4Source}
\lstinputlisting[language=Python]{q4.py}

\section{Question 5 source}\label{Q5Source}
\lstinputlisting[language=Haskell]{q5.hs}

\end{document}